SPARK_MAJOR_VERSION is set to 2, using Spark2
21/07/13 00:23:22 INFO SparkContext: Running Spark version 2.3.1.3.0.1.0-187
21/07/13 00:23:22 INFO SparkContext: Submitted application: My Application
21/07/13 00:23:22 INFO SecurityManager: Changing view acls to: root
21/07/13 00:23:22 INFO SecurityManager: Changing modify acls to: root
21/07/13 00:23:22 INFO SecurityManager: Changing view acls groups to: 
21/07/13 00:23:22 INFO SecurityManager: Changing modify acls groups to: 
21/07/13 00:23:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
21/07/13 00:23:23 INFO Utils: Successfully started service 'sparkDriver' on port 34573.
21/07/13 00:23:23 INFO SparkEnv: Registering MapOutputTracker
21/07/13 00:23:23 INFO SparkEnv: Registering BlockManagerMaster
21/07/13 00:23:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/13 00:23:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/13 00:23:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ddfefa26-b731-49bf-9176-0d25b54f16ea
21/07/13 00:23:23 INFO MemoryStore: MemoryStore started with capacity 93.3 MB
21/07/13 00:23:23 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/13 00:23:23 INFO log: Logging initialized @5397ms
21/07/13 00:23:23 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
21/07/13 00:23:23 INFO Server: Started @5532ms
21/07/13 00:23:23 INFO AbstractConnector: Started ServerConnector@6fcd26d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21/07/13 00:23:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ac4dda6{/jobs,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@76777a63{/jobs/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2afe7681{/jobs/job,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2f385d0{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f1a538f{/stages,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16097e93{/stages/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ad13844{/stages/stage,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@64a942a9{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57894560{/stages/pool,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@64164cf5{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@171b98bc{/storage,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@39893cfc{/storage/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2436cce8{/storage/rdd,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72914d7f{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3904e5e9{/environment,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@499a50e9{/environment/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bd994be{/executors,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@23dbc00f{/executors/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3dee4230{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26e8407c{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31835c7b{/static,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a3c45c6{/,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@b09fa7d{/api,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62dbd931{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3b9a33ff{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/13 00:23:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://sandbox-hdp.hortonworks.com:4040
21/07/13 00:23:24 INFO Executor: Starting executor ID driver on host localhost
21/07/13 00:23:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38525.
21/07/13 00:23:24 INFO NettyBlockTransferService: Server created on sandbox-hdp.hortonworks.com:38525
21/07/13 00:23:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/13 00:23:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, sandbox-hdp.hortonworks.com, 38525, None)
21/07/13 00:23:24 INFO BlockManagerMasterEndpoint: Registering block manager sandbox-hdp.hortonworks.com:38525 with 93.3 MB RAM, BlockManagerId(driver, sandbox-hdp.hortonworks.com, 38525, None)
21/07/13 00:23:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, sandbox-hdp.hortonworks.com, 38525, None)
21/07/13 00:23:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, sandbox-hdp.hortonworks.com, 38525, None)
21/07/13 00:23:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45bea15e{/metrics/json,null,AVAILABLE,@Spark}
21/07/13 00:23:27 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/local-1626135803886
21/07/13 00:23:28 INFO SharedState: loading hive config file: file:/etc/spark2/3.0.1.0-187/0/hive-site.xml
21/07/13 00:23:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/apps/spark/warehouse').
21/07/13 00:23:28 INFO SharedState: Warehouse path is '/apps/spark/warehouse'.
21/07/13 00:23:28 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@29a08cd0{/SQL,null,AVAILABLE,@Spark}
21/07/13 00:23:28 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a280cc3{/SQL/json,null,AVAILABLE,@Spark}
21/07/13 00:23:28 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@169c7709{/SQL/execution,null,AVAILABLE,@Spark}
21/07/13 00:23:28 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53d3795b{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/13 00:23:28 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c2edece{/static/sql,null,AVAILABLE,@Spark}
21/07/13 00:23:29 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/07/13 00:23:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 363.6 KB, free 92.9 MB)
21/07/13 00:23:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.8 KB, free 92.9 MB)
21/07/13 00:23:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on sandbox-hdp.hortonworks.com:38525 (size: 30.8 KB, free: 93.3 MB)
21/07/13 00:23:31 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
21/07/13 00:23:32 INFO FileInputFormat: Total input files to process : 1
21/07/13 00:23:32 INFO SparkContext: Starting job: runJob at PythonRDD.scala:149
21/07/13 00:23:32 INFO DAGScheduler: Registering RDD 3 (groupByKey at /root/spark_autoinc.py:86)
21/07/13 00:23:32 INFO DAGScheduler: Registering RDD 7 (reduceByKey at /root/spark_autoinc.py:92)
21/07/13 00:23:32 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:149) with 1 output partitions
21/07/13 00:23:32 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:149)
21/07/13 00:23:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
21/07/13 00:23:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
21/07/13 00:23:32 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /root/spark_autoinc.py:86), which has no missing parents
21/07/13 00:23:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.4 KB, free 92.9 MB)
21/07/13 00:23:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KB, free 92.9 MB)
21/07/13 00:23:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on sandbox-hdp.hortonworks.com:38525 (size: 6.6 KB, free: 93.3 MB)
21/07/13 00:23:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
21/07/13 00:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /root/spark_autoinc.py:86) (first 15 tasks are for partitions Vector(0))
21/07/13 00:23:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/07/13 00:23:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7901 bytes)
21/07/13 00:23:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/07/13 00:23:33 INFO HadoopRDD: Input split: hdfs://sandbox-hdp.hortonworks.com:8020/user/admin/spark_data.csv:0+993
21/07/13 00:23:35 INFO PythonRunner: Times: total = 1027, boot = 852, init = 168, finish = 7
21/07/13 00:23:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1776 bytes result sent to driver
21/07/13 00:23:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2079 ms on localhost (executor driver) (1/1)
21/07/13 00:23:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/07/13 00:23:35 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57517
21/07/13 00:23:35 INFO DAGScheduler: ShuffleMapStage 0 (groupByKey at /root/spark_autoinc.py:86) finished in 2.777 s
21/07/13 00:23:35 INFO DAGScheduler: looking for newly runnable stages
21/07/13 00:23:35 INFO DAGScheduler: running: Set()
21/07/13 00:23:35 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
21/07/13 00:23:35 INFO DAGScheduler: failed: Set()
21/07/13 00:23:35 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at reduceByKey at /root/spark_autoinc.py:92), which has no missing parents
21/07/13 00:23:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.5 KB, free 92.9 MB)
21/07/13 00:23:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KB, free 92.9 MB)
21/07/13 00:23:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on sandbox-hdp.hortonworks.com:38525 (size: 7.3 KB, free: 93.3 MB)
21/07/13 00:23:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
21/07/13 00:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at reduceByKey at /root/spark_autoinc.py:92) (first 15 tasks are for partitions Vector(0))
21/07/13 00:23:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/07/13 00:23:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
21/07/13 00:23:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/07/13 00:23:35 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
21/07/13 00:23:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
21/07/13 00:23:35 INFO PythonRunner: Times: total = 48, boot = -666, init = 713, finish = 1
21/07/13 00:23:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1905 bytes result sent to driver
21/07/13 00:23:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 223 ms on localhost (executor driver) (1/1)
21/07/13 00:23:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/07/13 00:23:35 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /root/spark_autoinc.py:92) finished in 0.256 s
21/07/13 00:23:35 INFO DAGScheduler: looking for newly runnable stages
21/07/13 00:23:35 INFO DAGScheduler: running: Set()
21/07/13 00:23:35 INFO DAGScheduler: waiting: Set(ResultStage 2)
21/07/13 00:23:35 INFO DAGScheduler: failed: Set()
21/07/13 00:23:35 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[10] at RDD at PythonRDD.scala:49), which has no missing parents
21/07/13 00:23:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.5 KB, free 92.9 MB)
21/07/13 00:23:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KB, free 92.9 MB)
21/07/13 00:23:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on sandbox-hdp.hortonworks.com:38525 (size: 4.7 KB, free: 93.3 MB)
21/07/13 00:23:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039
21/07/13 00:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[10] at RDD at PythonRDD.scala:49) (first 15 tasks are for partitions Vector(0))
21/07/13 00:23:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/07/13 00:23:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
21/07/13 00:23:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/07/13 00:23:36 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
21/07/13 00:23:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/07/13 00:23:36 INFO PythonRunner: Times: total = 51, boot = -206, init = 256, finish = 1
21/07/13 00:23:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1781 bytes result sent to driver
21/07/13 00:23:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 139 ms on localhost (executor driver) (1/1)
21/07/13 00:23:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/07/13 00:23:36 INFO DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:149) finished in 0.190 s
21/07/13 00:23:36 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:149, took 3.712557 s
21/07/13 00:23:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on sandbox-hdp.hortonworks.com:38525 in memory (size: 4.7 KB, free: 93.3 MB)
21/07/13 00:23:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on sandbox-hdp.hortonworks.com:38525 in memory (size: 7.3 KB, free: 93.3 MB)
21/07/13 00:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/07/13 00:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/07/13 00:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/07/13 00:23:39 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
21/07/13 00:23:39 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/07/13 00:23:39 INFO DAGScheduler: Final stage: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0)
21/07/13 00:23:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
21/07/13 00:23:39 INFO DAGScheduler: Missing parents: List()
21/07/13 00:23:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
21/07/13 00:23:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 209.0 KB, free 92.7 MB)
21/07/13 00:23:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 74.7 KB, free 92.6 MB)
21/07/13 00:23:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on sandbox-hdp.hortonworks.com:38525 (size: 74.7 KB, free: 93.2 MB)
21/07/13 00:23:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1039
21/07/13 00:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/07/13 00:23:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/07/13 00:23:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3, localhost, executor driver, partition 0, ANY, 7649 bytes)
21/07/13 00:23:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)
21/07/13 00:23:39 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
21/07/13 00:23:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/07/13 00:23:40 INFO CodeGenerator: Code generated in 471.834545 ms
21/07/13 00:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/07/13 00:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/07/13 00:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/07/13 00:23:40 INFO PythonRunner: Times: total = 43, boot = -3315, init = 3358, finish = 0
21/07/13 00:23:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210713002340_0005_m_000000_0' to hdfs://sandbox-hdp.hortonworks.com:8020/user/admin/spark_output
21/07/13 00:23:41 INFO SparkHadoopMapRedUtil: attempt_20210713002340_0005_m_000000_0: Committed
21/07/13 00:23:41 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 3002 bytes result sent to driver
21/07/13 00:23:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 1693 ms on localhost (executor driver) (1/1)
21/07/13 00:23:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/07/13 00:23:41 INFO DAGScheduler: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 1.846 s
21/07/13 00:23:41 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 1.856553 s
21/07/13 00:23:41 INFO FileFormatWriter: Job null committed.
21/07/13 00:23:41 INFO FileFormatWriter: Finished processing stats for job null.
21/07/13 00:23:41 INFO SparkContext: Invoking stop() from shutdown hook
21/07/13 00:23:42 INFO AbstractConnector: Stopped Spark@6fcd26d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21/07/13 00:23:42 INFO SparkUI: Stopped Spark web UI at http://sandbox-hdp.hortonworks.com:4040
21/07/13 00:23:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/07/13 00:23:42 INFO MemoryStore: MemoryStore cleared
21/07/13 00:23:42 INFO BlockManager: BlockManager stopped
21/07/13 00:23:42 INFO BlockManagerMaster: BlockManagerMaster stopped
21/07/13 00:23:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/07/13 00:23:42 INFO SparkContext: Successfully stopped SparkContext
21/07/13 00:23:42 INFO ShutdownHookManager: Shutdown hook called
21/07/13 00:23:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2dda3d7-23ac-41be-bee5-257d9b01d615
21/07/13 00:23:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-dff69e51-415d-4260-8f7a-9395998193f6
21/07/13 00:23:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-dff69e51-415d-4260-8f7a-9395998193f6/pyspark-4cfd86bc-ab9c-4553-9cfc-3f7d5d0c644d
